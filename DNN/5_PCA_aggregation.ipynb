{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA dimensional reduction\n",
    "\n",
    "Given the limited number of training samples (approximately 13k), it is advisable to decrease the number of features in order to reduce the total number of parameters required for a deep learning model. Therefore, to mitigate the \"curse of dimensionality\" while preserving the relevant information, PCA will be employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Functionality Breakdown**\n",
    "\n",
    "#### **1. Purpose of the Code**\n",
    "This code performs **PCA (Principal Component Analysis)** on different extracted audio features, such as acoustic indices, MPS, YAMNet, VGGish, and PANNs embeddings. It aims to reduce dimensionality, retain significant variance, and save transformed features for further analysis and modeling in your **speech emotion recognition project**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Components**\n",
    "\n",
    "#### **`PCAProcessor` Class**\n",
    "The class encapsulates the workflow for:\n",
    "- **Loading Features:** Loads the extracted features from disk.\n",
    "- **PCA Transformation:** Reduces dimensionality of each feature set while preserving the desired variance.\n",
    "- **Visualization:** Generates plots for variance explained and feature correlations.\n",
    "- **Saving Results:** Saves transformed features and PCA models.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.1 Initialization**\n",
    "- **`data_dir`:** Base directory containing the dataset and feature files.\n",
    "- **`processed_dir`:** Directory where PCA models and transformed data are stored.\n",
    "- **`visualization_dir`:** Directory for saving visualizations of PCA results.\n",
    "- **`feature_configs`:** Configurations for each feature set, including:\n",
    "  - **Name:** Human-readable identifier for the feature.\n",
    "  - **Number of Components (`n_components`):** Maximum PCA components to retain.\n",
    "  - **`handle_inf`:** Whether to handle infinities by replacing them with NaNs.\n",
    "  - **`min_explained_variance`:** Minimum cumulative variance required (e.g., 95%).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.2 Feature Loading**\n",
    "- **`load_data`:**\n",
    "  - Loads the metadata (`train_val_test_split_EMODB.csv`) and feature files (e.g., `yamnet_embedding.npy`, `indices_raw.csv`).\n",
    "  - Checks that all feature files exist and have the correct length.\n",
    "\n",
    "- **`_validate_feature_lengths`:**\n",
    "  - Ensures that the length of each feature set matches the number of samples in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.3 PCA Transformation**\n",
    "- **`get_transformer`:**\n",
    "  - Creates a **PCA pipeline** with an imputer (to handle NaNs), PCA transformer, and a scaler.\n",
    "\n",
    "- **`analyze_variance_explained`:**\n",
    "  - Visualizes the explained variance for PCA components:\n",
    "    - **Individual Variance:** Variance explained by each component.\n",
    "    - **Cumulative Variance:** Total variance explained by the top `n` components.\n",
    "\n",
    "- **`transform_feature`:**\n",
    "  - Transforms a single feature set:\n",
    "    1. **Handles NaNs/Infinities:** Replaces missing or infinite values with the mean.\n",
    "    2. **Fits PCA on Training Data:** Retains only the required number of components.\n",
    "    3. **Transforms All Splits:** Applies the fitted PCA model to training, validation, and test sets.\n",
    "    4. **Saves PCA Model:** Saves the PCA transformer for later use.\n",
    "    5. **Variance Analysis:** Generates plots for variance explained.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.4 Correlation Analysis**\n",
    "- **`visualize_correlations`:**\n",
    "  - Analyzes correlations between transformed feature sets for a given split (e.g., training data).\n",
    "  - Creates a **heatmap** of mean absolute correlations between feature sets, providing insights into redundancy or complementarity.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.5 Feature Processing**\n",
    "- **`process_features`:**\n",
    "  - End-to-end workflow for processing all feature sets:\n",
    "    1. **Load Data:** Loads dataset and feature files.\n",
    "    2. **Define Splits:** Creates masks for training, validation, and test sets based on the fold column.\n",
    "    3. **Transform Features:** Applies PCA transformation to each feature set.\n",
    "    4. **Visualize Correlations:** Generates a heatmap of feature correlations.\n",
    "    5. **Save Results:** Saves the aggregated transformed data and labels as a `.pkl` file.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Main Workflow**\n",
    "\n",
    "#### **`main` Function**\n",
    "- **Purpose:** Defines the entry point for running the PCA processing workflow.\n",
    "- **Steps:**\n",
    "  1. Initializes the `PCAProcessor` with paths and parameters.\n",
    "  2. Calls `process_features` to process all feature sets.\n",
    "  3. Logs the completion or errors.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Role in Your Project**\n",
    "\n",
    "This code contributes significantly to your **speech emotion recognition project** by:\n",
    "1. **Dimensionality Reduction:**\n",
    "   - Reduces the size of feature sets (e.g., embeddings) while preserving essential information.\n",
    "   - Helps avoid overfitting and reduces computational costs during modeling.\n",
    "\n",
    "2. **Explained Variance Analysis:**\n",
    "   - Provides insights into how many components are needed to retain a specific amount of variance.\n",
    "   - Ensures an optimal balance between dimensionality and information retention.\n",
    "\n",
    "3. **Correlation Analysis:**\n",
    "   - Identifies redundancies or complementarities between feature sets.\n",
    "   - Guides feature selection or engineering decisions.\n",
    "\n",
    "4. **Standardized Data:** \n",
    "   - Prepares uniformly transformed feature sets for downstream modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Outputs**\n",
    "1. **Transformed Features:** Saved as `.pkl` or `.npy` files.\n",
    "2. **PCA Models:** Saved as `.pkl` for reuse.\n",
    "3. **Visualizations:**\n",
    "   - **Variance Explained:** For each feature set.\n",
    "   - **Feature Correlations:** Heatmaps showing relationships between feature sets.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you’d like additional explanations, optimizations, or extensions to this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:01:59,357 - INFO - Loaded dataset with 535 samples\n",
      "2024-12-07 17:01:59,361 - INFO - Loaded indices_raw with shape (535, 33)\n",
      "2024-12-07 17:01:59,368 - INFO - Loaded mps with shape (535, 8080)\n",
      "2024-12-07 17:01:59,370 - INFO - Loaded yamnet with shape (535, 1024)\n",
      "2024-12-07 17:01:59,372 - INFO - Loaded vggish with shape (535, 128)\n",
      "2024-12-07 17:01:59,372 - WARNING - Missing feature files: ['/Users/huangjuhua/文档文稿/NYU/Time_Series/data/processed/embeddings/panns_clip.npy', '/Users/huangjuhua/文档文稿/NYU/Time_Series/data/processed/embeddings/panns_embedding.npy']\n",
      "Processing features:   0%|          | 0/6 [00:00<?, ?it/s]2024-12-07 17:01:59,803 - INFO - Acoustic Indices: 1 components needed for 95.0% explained variance\n",
      "Processing features:  17%|█▋        | 1/6 [00:00<00:02,  2.40it/s]2024-12-07 17:02:03,453 - INFO - MPS Features: 7 components needed for 90.0% explained variance\n",
      "Processing features:  33%|███▎      | 2/6 [00:04<00:09,  2.32s/it]2024-12-07 17:02:06,181 - INFO - YAMNet: 1 components needed for 95.0% explained variance\n",
      "Processing features:  50%|█████     | 3/6 [00:06<00:07,  2.51s/it]2024-12-07 17:02:06,660 - INFO - VGGish: 24 components needed for 95.0% explained variance\n",
      "Processing features: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it]\n",
      "2024-12-07 17:02:06,816 - INFO - Saved aggregated data to /Users/huangjuhua/文档文稿/NYU/Time_Series/data/processed/aggregated_data.pkl\n",
      "2024-12-07 17:02:06,818 - INFO - PCA processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from typing import Dict, Tuple, List, Any, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pca_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"Configuration for PCA transformation of different features\"\"\"\n",
    "    name: str\n",
    "    n_components: int\n",
    "    handle_inf: bool = False\n",
    "    min_explained_variance: float = 0.95\n",
    "\n",
    "class PCAProcessor:\n",
    "    \"\"\"Class to handle PCA transformations of audio features\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_dir: str, \n",
    "                 random_state: int = 23,\n",
    "                 visualization_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize PCA processor\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Base directory for data\n",
    "            random_state: Random seed for reproducibility\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.processed_dir = self.data_dir / 'processed'\n",
    "        self.pca_dir = self.processed_dir / 'PCA_transformer'\n",
    "        self.visualization_dir = Path(visualization_dir) if visualization_dir else self.processed_dir / 'visualizations'\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Create necessary directories\n",
    "        self.pca_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.visualization_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define feature configurations\n",
    "        self.feature_configs = {\n",
    "            'indices_raw': FeatureConfig('Acoustic Indices', 20, True, 0.95),\n",
    "            'mps': FeatureConfig('MPS Features', 10, False, 0.9),\n",
    "            'yamnet': FeatureConfig('YAMNet', 100, False, 0.95),\n",
    "            'vggish': FeatureConfig('VGGish', 64, False, 0.95),\n",
    "            'panns_clip': FeatureConfig('PANNs Clip', 50, False, 0.95),\n",
    "            'panns_embedding': FeatureConfig('PANNs Embedding', 100, False, 0.95)\n",
    "        }\n",
    "        \n",
    "    def load_data(self) -> Tuple[pd.DataFrame, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Load all required data files with validation\"\"\"\n",
    "        try:\n",
    "            # Load dataset splits\n",
    "            df_all = pd.read_csv(self.data_dir / 'train_val_test_split_EMODB.csv')\n",
    "            logging.info(f\"Loaded dataset with {len(df_all)} samples\")\n",
    "            \n",
    "            # Define feature files\n",
    "            feature_files = {\n",
    "                'indices_raw': self.processed_dir / 'embeddings' / 'indices_raw.csv',\n",
    "                'mps': self.processed_dir / 'embeddings' / 'mps_embedding.npy',\n",
    "                'yamnet': self.processed_dir / 'embeddings' / 'yamnet_embedding.npy',\n",
    "                'vggish': self.processed_dir / 'embeddings' / 'vggish_embedding.npy',\n",
    "                'panns_clip': self.processed_dir / 'embeddings' / 'panns_clip.npy',\n",
    "                'panns_embedding': self.processed_dir / 'embeddings' / 'panns_embedding.npy'\n",
    "            }\n",
    "            \n",
    "            # Load features\n",
    "            features = {}\n",
    "            missing_files = []\n",
    "            \n",
    "            for feature_name, file_path in feature_files.items():\n",
    "                if not file_path.exists():\n",
    "                    missing_files.append(str(file_path))\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    if str(file_path).endswith('.csv'):\n",
    "                        features[feature_name] = pd.read_csv(file_path)\n",
    "                    else:\n",
    "                        features[feature_name] = np.load(file_path)\n",
    "                        \n",
    "                    logging.info(f\"Loaded {feature_name} with shape {features[feature_name].shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error loading {feature_name}: {str(e)}\")\n",
    "                    missing_files.append(str(file_path))\n",
    "            \n",
    "            if missing_files:\n",
    "                logging.warning(f\"Missing feature files: {missing_files}\")\n",
    "                \n",
    "            # Validate data lengths\n",
    "            self._validate_feature_lengths(features, len(df_all))\n",
    "            \n",
    "            return df_all, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _validate_feature_lengths(self, features: Dict[str, np.ndarray], expected_length: int) -> None:\n",
    "        \"\"\"Validate that all features have the correct length\"\"\"\n",
    "        for name, feature in features.items():\n",
    "            actual_length = len(feature) if isinstance(feature, pd.DataFrame) else feature.shape[0]\n",
    "            if actual_length != expected_length:\n",
    "                raise ValueError(f\"Feature {name} has length {actual_length}, expected {expected_length}\")\n",
    "                \n",
    "    def get_transformer(self, n_components: int) -> Pipeline:\n",
    "        \"\"\"Create PCA transformer pipeline\"\"\"\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "            ('pca', PCA(n_components=n_components, random_state=self.random_state)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "    def analyze_variance_explained(self, \n",
    "                                 transformer: Pipeline, \n",
    "                                 feature_name: str,\n",
    "                                 min_explained_variance: float) -> None:\n",
    "        \"\"\"Analyze and visualize PCA explained variance\"\"\"\n",
    "        pca = transformer.named_steps['pca']\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "        \n",
    "        # Plot explained variance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Individual variance\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(explained_variance_ratio, 'b-', label='Individual')\n",
    "        plt.xlabel('Principal Components')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title(f'{feature_name} - Individual Explained Variance')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Cumulative variance\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(cumulative_variance_ratio, 'r-', label='Cumulative')\n",
    "        plt.axhline(y=min_explained_variance, color='g', linestyle='--', \n",
    "                   label=f'{min_explained_variance*100}% Threshold')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "        plt.title(f'{feature_name} - Cumulative Explained Variance')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.visualization_dir / f'variance_explained_{feature_name}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Log variance information\n",
    "        n_components_95 = np.argmax(cumulative_variance_ratio >= min_explained_variance) + 1\n",
    "        logging.info(f\"{feature_name}: {n_components_95} components needed for \"\n",
    "                    f\"{min_explained_variance*100}% explained variance\")\n",
    "        \n",
    "    def transform_feature(self, \n",
    "                         data: np.ndarray,\n",
    "                         splits: Dict[str, np.ndarray],\n",
    "                         config: FeatureConfig) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Transform a single feature set\"\"\"\n",
    "        try:\n",
    "            # Handle infinities if needed\n",
    "            if config.handle_inf:\n",
    "                if isinstance(data, pd.DataFrame):\n",
    "                    data = data.replace([np.inf, -np.inf], np.nan).values\n",
    "                else:\n",
    "                    data = np.nan_to_num(data, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
    "            \n",
    "            # Get transformer and fit on training data\n",
    "            transformer = self.get_transformer(config.n_components)\n",
    "            transformer.fit(data[splits['train']])\n",
    "            \n",
    "            # Transform all splits\n",
    "            transformed = {\n",
    "                split_name: transformer.transform(data[split_mask])\n",
    "                for split_name, split_mask in splits.items()\n",
    "            }\n",
    "            \n",
    "            # Save transformer\n",
    "            with open(self.pca_dir / f'{config.name.lower()}_transformer.pkl', 'wb') as f:\n",
    "                pickle.dump(transformer, f)\n",
    "                \n",
    "            # Analyze variance explained\n",
    "            self.analyze_variance_explained(\n",
    "                transformer, \n",
    "                config.name,\n",
    "                config.min_explained_variance\n",
    "            )\n",
    "            \n",
    "            return transformed\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error transforming {config.name}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def visualize_correlations(self, \n",
    "                             transformed_data: Dict[str, np.ndarray],\n",
    "                             split: str = 'train') -> None:\n",
    "        \"\"\"Visualize correlations between different feature sets\"\"\"\n",
    "        try:\n",
    "            # Collect all transformed features for the specified split\n",
    "            features_dict = {\n",
    "                name.replace(f'{split}_', '').replace('_pca', ''): data\n",
    "                for name, data in transformed_data.items()\n",
    "                if name.startswith(f'{split}_') and name.endswith('_pca')\n",
    "            }\n",
    "            \n",
    "            # Calculate mean correlations between feature sets\n",
    "            n_features = len(features_dict)\n",
    "            correlation_matrix = np.zeros((n_features, n_features))\n",
    "            feature_names = list(features_dict.keys())\n",
    "            \n",
    "            for i, (name1, features1) in enumerate(features_dict.items()):\n",
    "                for j, (name2, features2) in enumerate(features_dict.items()):\n",
    "                    if i <= j:\n",
    "                        # Calculate mean absolute correlation\n",
    "                        corr = np.mean(np.abs(np.corrcoef(features1.T, features2.T)))\n",
    "                        correlation_matrix[i, j] = corr\n",
    "                        correlation_matrix[j, i] = corr\n",
    "            \n",
    "            # Plot correlation matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(correlation_matrix, \n",
    "                       xticklabels=feature_names,\n",
    "                       yticklabels=feature_names,\n",
    "                       cmap='coolwarm',\n",
    "                       annot=True,\n",
    "                       fmt='.2f')\n",
    "            plt.title(f'Mean Feature Correlations ({split} split)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.visualization_dir / f'feature_correlations_{split}.png')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error visualizing correlations: {str(e)}\")\n",
    "            \n",
    "    def process_features(self) -> Dict[str, Any]:\n",
    "        \"\"\"Process all features with PCA\"\"\"\n",
    "        try:\n",
    "            # Load data\n",
    "            df_all, features = self.load_data()\n",
    "            \n",
    "            if not features:\n",
    "                raise ValueError(\"No features available for processing\")\n",
    "            \n",
    "            # Define splits\n",
    "            splits = {\n",
    "                'train': df_all['fold'].isin([2, 3, 4]),\n",
    "                'valid': df_all['fold'] == 1,\n",
    "                'test': df_all['fold'] == 0\n",
    "            }\n",
    "            \n",
    "            # Transform available features\n",
    "            transformed_data = {}\n",
    "            for feature_name, config in tqdm(self.feature_configs.items(), \n",
    "                                          desc=\"Processing features\"):\n",
    "                if feature_name in features:\n",
    "                    transformed = self.transform_feature(\n",
    "                        features[feature_name],\n",
    "                        splits,\n",
    "                        config\n",
    "                    )\n",
    "                    \n",
    "                    for split_name, data in transformed.items():\n",
    "                        transformed_data[f'{split_name}_{feature_name}_pca'] = data\n",
    "            \n",
    "            # Add labels\n",
    "            for split_name, split_mask in splits.items():\n",
    "                transformed_data[f'y_{split_name}'] = df_all.loc[split_mask, 'emotion_label']\n",
    "            \n",
    "            # Visualize feature correlations\n",
    "            self.visualize_correlations(transformed_data, 'train')\n",
    "            \n",
    "            # Save aggregated data\n",
    "            save_path = self.processed_dir / 'aggregated_data.pkl'\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(transformed_data, f)\n",
    "                \n",
    "            logging.info(f\"Saved aggregated data to {save_path}\")\n",
    "            \n",
    "            return transformed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in feature processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        data_dir = '/Users/huangjuhua/文档文稿/NYU/Time_Series/data'\n",
    "        \n",
    "        processor = PCAProcessor(\n",
    "            data_dir=data_dir,\n",
    "            random_state=23,\n",
    "            visualization_dir=os.path.join(data_dir, 'processed', 'visualizations')\n",
    "        )\n",
    "        \n",
    "        processor.process_features()\n",
    "        logging.info(\"PCA processing completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "time_series_finalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
