{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92727a9b",
   "metadata": {},
   "source": [
    "# Convert audio signals to pre-trained CNN embeddings\n",
    "\n",
    "The audio signals are being converted to pre-trained CNN embeddings, which will be used for classification tasks. This process involves extracting high-level features from the audio data using pre-trained convolutional neural network models.\n",
    "\n",
    "Three popular pre-trained CNN models for classifying audio events have been selected:\n",
    "- VGGish: https://github.com/tensorflow/models/tree/master/research/audioset/vggish\n",
    "- YAMNet: https://github.com/tensorflow/models/tree/master/research/audioset/yamnet\n",
    "- PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab565d98",
   "metadata": {},
   "source": [
    "### **Code Functionality Breakdown**\n",
    "\n",
    "#### **1. Purpose of the Code**\n",
    "This code is a pipeline for **extracting audio embeddings** using pre-trained models (YAMNet, VGGish, and PANNs) and calculating **acoustic features** for your **speech emotion recognition project**. It processes `.pkl` files containing preprocessed audio data, extracts embeddings, and saves them for further use in machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Components**\n",
    "\n",
    "#### **`AudioEmbeddingExtractor` Class**\n",
    "This class encapsulates the entire workflow for:\n",
    "- Extracting embeddings from pre-trained models.\n",
    "- Calculating acoustic features like MFCCs and spectral indices.\n",
    "- Saving the extracted features.\n",
    "\n",
    "##### **Attributes**\n",
    "1. **`csv_path`**: Path to the metadata CSV containing `.pkl` file paths.\n",
    "2. **`output_dir`**: Directory where extracted embeddings will be saved.\n",
    "3. **`sample_rate`**: The sampling rate of the audio (default: 16 kHz).\n",
    "4. **`target_length`**: Length to which all audio signals are padded or truncated.\n",
    "5. **`batch_size`**: Batch size for processing PANN embeddings.\n",
    "6. **`use_gpu`**: Flag to enable GPU for PANN inference.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.1 Initialization and Pre-trained Model Loading**\n",
    "- **`_load_models`**: Loads the pre-trained models:\n",
    "  1. **YAMNet**: Extracts embeddings for audio classification tasks.\n",
    "  2. **VGGish**: Generates audio embeddings.\n",
    "  3. **PANNs**: Extracts clip-level and frame-level audio embeddings.\n",
    "  \n",
    "If a model fails to load, the process logs an error and halts.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.2 Feature Calculation**\n",
    "- **`calculate_acoustic_features`**: Extracts basic acoustic features like:\n",
    "  - RMS energy, zero-crossing rate, spectral centroid, rolloff, and bandwidth.\n",
    "  - MFCCs (mean and standard deviation of 13 coefficients).\n",
    "  - Spectral contrast and flatness.\n",
    "\n",
    "These features provide insights into the audio signal’s characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.3 Audio Preprocessing**\n",
    "- **`load_and_preprocess_audio`**:\n",
    "  1. Loads the `.pkl` file containing preprocessed audio (`y`).\n",
    "  2. Pads or truncates the audio to the `target_length`.\n",
    "  3. Calculates acoustic features if not already present.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.4 Embedding Extraction**\n",
    "- **`extract_yamnet_vggish_features`**:\n",
    "  - Extracts embeddings using YAMNet and VGGish models.\n",
    "  - Processes **MPS features** if available in the input data.\n",
    "  \n",
    "- **`process_panns_embeddings`**:\n",
    "  - Resamples the audio to 32 kHz (required for PANNs).\n",
    "  - Processes audio in batches for efficiency.\n",
    "  - Extracts clip-level and frame-level embeddings using PANNs.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.5 Dataset Processing**\n",
    "- **`process_dataset`**:\n",
    "  1. Loads the dataset metadata from the provided CSV file.\n",
    "  2. Iterates through `.pkl` file paths to:\n",
    "     - Preprocess audio.\n",
    "     - Extract embeddings using YAMNet, VGGish, and PANNs.\n",
    "     - Collect features like MPS and acoustic indices.\n",
    "  3. Periodically runs garbage collection to manage memory.\n",
    "  4. Saves all features (embeddings and indices) to the output directory.\n",
    "\n",
    "- **`_save_features`**:\n",
    "  - Saves extracted features as `.npy` files (for embeddings) and `.csv` files (for acoustic indices).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Main Function**\n",
    "The `main()` function defines the workflow:\n",
    "1. Sets the paths for the dataset metadata and output directory.\n",
    "2. Initializes the `AudioEmbeddingExtractor`.\n",
    "3. Runs the `process_dataset()` method to extract embeddings and save them.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Role in Your Project**\n",
    "This code plays a **critical role** in your speech emotion recognition project by:\n",
    "- **Generating Input Features**:\n",
    "  - Extracts embeddings from pre-trained models (YAMNet, VGGish, PANNs) and calculates acoustic features.\n",
    "  - These features form the **input for your deep learning models** (CNN or DNN).\n",
    "  \n",
    "- **Standardizing Audio Data**:\n",
    "  - Pads or truncates audio signals to a consistent length for uniform model input.\n",
    "  \n",
    "- **Handling Large Data**:\n",
    "  - Processes data in batches and uses memory management techniques to handle large datasets efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Workflow Overview**\n",
    "1. **Input Data**: `.pkl` files containing preprocessed audio signals.\n",
    "2. **Processing Steps**:\n",
    "   - Load and preprocess audio.\n",
    "   - Extract embeddings (YAMNet, VGGish, PANNs).\n",
    "   - Calculate acoustic features.\n",
    "3. **Output Data**:\n",
    "   - `.npy` files for embeddings (YAMNet, VGGish, PANNs).\n",
    "   - `.csv` file for acoustic features.\n",
    "\n",
    "Let me know if you’d like help customizing or optimizing specific parts of this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5e4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:21:38,205 - INFO - Loading pre-trained models...\n",
      "2024-12-07 16:21:38,206 - INFO - Using /var/folders/8d/r58q4mcx1sv_vq93k0x2r2dw0000gn/T/tfhub_modules to cache modules.\n",
      "2024-12-07 16:21:42,223 - INFO - Fingerprint not found. Saved model loading will continue.\n",
      "2024-12-07 16:21:42,224 - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
      "2024-12-07 16:21:42,481 - INFO - Fingerprint not found. Saved model loading will continue.\n",
      "2024-12-07 16:21:42,482 - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /Users/huangjuhua/panns_data/Cnn14_mAP=0.431.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:21:43,717 - INFO - Models loaded successfully\n",
      "2024-12-07 16:21:43,721 - INFO - Processing 535 audio files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 535/535 [02:14<00:00,  3.97it/s]\n",
      "2024-12-07 16:23:58,566 - INFO - Resampling audio for PANNs processing...\n",
      "Processing PANNs batches: 100%|██████████| 6/6 [00:56<00:00,  9.42s/it]\n",
      "2024-12-07 16:24:55,965 - INFO - Features saved to /Users/huangjuhua/文档文稿/NYU/Time_Series/data/processed/embeddings\n",
      "2024-12-07 16:24:55,966 - INFO - Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "import pickle\n",
    "import time\n",
    "from panns_inference import AudioTagging\n",
    "from typing import Dict, List, Any, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('audio_embeddings.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class AudioEmbeddingExtractor:\n",
    "    \"\"\"Class to handle extraction of audio embeddings using multiple pre-trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 csv_path: str,\n",
    "                 output_dir: str,\n",
    "                 sample_rate: int = 16000,\n",
    "                 target_length: int = 160000,\n",
    "                 batch_size: int = 100,\n",
    "                 use_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the embedding extractor\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to CSV containing audio file paths\n",
    "            output_dir: Directory to save embeddings\n",
    "            sample_rate: Original sample rate of audio\n",
    "            target_length: Target length for padding/truncating\n",
    "            batch_size: Batch size for PANN processing\n",
    "            use_gpu: Whether to use GPU for PANN inference\n",
    "        \"\"\"\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_length = target_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = 'cuda' if use_gpu else 'cpu'\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize models\n",
    "        self._load_models()\n",
    "        \n",
    "    def _load_models(self) -> None:\n",
    "        \"\"\"Load all pre-trained models\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Loading pre-trained models...\")\n",
    "            self.yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "            self.vggish_model = hub.load('https://tfhub.dev/google/vggish/1')\n",
    "            self.panns_model = AudioTagging(checkpoint_path=None, device=self.device)\n",
    "            logging.info(\"Models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_acoustic_features(self, y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive acoustic features\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            features = {\n",
    "                'rms': np.sqrt(np.mean(y**2)),\n",
    "                'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
    "                'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=self.sample_rate)[0]),\n",
    "                'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=self.sample_rate)[0]),\n",
    "                'spectral_bandwidth': np.mean(librosa.feature.spectral_bandwidth(y=y, sr=self.sample_rate)[0])\n",
    "            }\n",
    "            \n",
    "            # MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=self.sample_rate, n_mfcc=13)\n",
    "            for i, mfcc in enumerate(mfccs):\n",
    "                features[f'mfcc_{i+1}_mean'] = np.mean(mfcc)\n",
    "                features[f'mfcc_{i+1}_std'] = np.std(mfcc)\n",
    "            \n",
    "            # Spectral features\n",
    "            features.update({\n",
    "                'spectral_contrast': np.mean(librosa.feature.spectral_contrast(y=y, sr=self.sample_rate)[0]),\n",
    "                'spectral_flatness': np.mean(librosa.feature.spectral_flatness(y=y))\n",
    "            })\n",
    "            \n",
    "            return pd.DataFrame([features])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating acoustic features: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_and_preprocess_audio(self, file_path: str) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load and preprocess a single audio file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                \n",
    "            # Check for required 'y' field\n",
    "            if 'y' not in data:\n",
    "                raise KeyError(\"Audio data 'y' not found in pickle file\")\n",
    "                \n",
    "            y = data['y']\n",
    "            \n",
    "            # Pad/truncate to target length\n",
    "            if len(y) < self.target_length:\n",
    "                pad_length = self.target_length - len(y)\n",
    "                y = np.pad(y, (0, pad_length), 'constant', constant_values=y.mean())\n",
    "            else:\n",
    "                y = y[:self.target_length]\n",
    "                \n",
    "            # Calculate acoustic features if not present\n",
    "            indices = data.get('df_indices', None)\n",
    "            if indices is None:\n",
    "                indices = self.calculate_acoustic_features(y)\n",
    "                \n",
    "            return {\n",
    "                'waveform': y,\n",
    "                'indices': indices,\n",
    "                'mps': data.get('mps', np.array([])),\n",
    "                'wt': data.get('wt', np.array([]))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_yamnet_vggish_features(self, audio_data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract YAMNet and VGGish embeddings\"\"\"\n",
    "        try:\n",
    "            y = audio_data['waveform']\n",
    "            \n",
    "            # YAMNet embeddings\n",
    "            scores, embedding_tensor, _ = self.yamnet_model(y)\n",
    "            yamnet_embedding = tf.reduce_mean(embedding_tensor, axis=0).numpy()\n",
    "            \n",
    "            # VGGish embeddings\n",
    "            vggish_embedding = tf.reduce_mean(self.vggish_model(y), axis=0).numpy()\n",
    "            \n",
    "            # Process MPS features if available\n",
    "            mps = audio_data.get('mps', np.array([]))\n",
    "            wt = audio_data.get('wt', np.array([]))\n",
    "            if mps.size > 0 and wt.size > 0:\n",
    "                mps = mps[:, wt <= 100].reshape(-1)\n",
    "                \n",
    "            return {\n",
    "                'yamnet': yamnet_embedding,\n",
    "                'vggish': vggish_embedding,\n",
    "                'mps': mps,\n",
    "                'indices': audio_data['indices']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting features: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def process_panns_embeddings(self, waveforms: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Process PANNs embeddings in batches\"\"\"\n",
    "        try:\n",
    "            # Resample to 32kHz for PANN\n",
    "            logging.info(\"Resampling audio for PANNs processing...\")\n",
    "            waveforms_32k = librosa.resample(\n",
    "                waveforms, \n",
    "                orig_sr=self.sample_rate,\n",
    "                target_sr=32000,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            clipwise_outputs = []\n",
    "            embeddings = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in tqdm(range(0, len(waveforms_32k), self.batch_size), \n",
    "                         desc=\"Processing PANNs batches\"):\n",
    "                batch = waveforms_32k[i:i + self.batch_size]\n",
    "                clipwise_output, embedding = self.panns_model.inference(batch)\n",
    "                clipwise_outputs.append(clipwise_output)\n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "            # Clean up to save memory\n",
    "            del waveforms_32k\n",
    "            gc.collect()\n",
    "            \n",
    "            return (np.concatenate(clipwise_outputs, axis=0),\n",
    "                   np.concatenate(embeddings, axis=0))\n",
    "                   \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing PANNs embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_dataset(self) -> None:\n",
    "        \"\"\"Process entire dataset and extract all embeddings\"\"\"\n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            logging.info(f\"Processing {len(df)} audio files\")\n",
    "            \n",
    "            # Initialize lists\n",
    "            waveforms = []\n",
    "            yamnet_embeddings = []\n",
    "            vggish_embeddings = []\n",
    "            mps_features = []\n",
    "            all_indices = []\n",
    "            failed_files = []\n",
    "            \n",
    "            # Process each file\n",
    "            for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing files\"):\n",
    "                try:\n",
    "                    # Load and process audio\n",
    "                    audio_data = self.load_and_preprocess_audio(row['pkl_path'])\n",
    "                    waveforms.append(audio_data['waveform'])\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = self.extract_yamnet_vggish_features(audio_data)\n",
    "                    \n",
    "                    # Collect features\n",
    "                    yamnet_embeddings.append(features['yamnet'])\n",
    "                    vggish_embeddings.append(features['vggish'])\n",
    "                    if features['mps'].size > 0:\n",
    "                        mps_features.append(features['mps'])\n",
    "                    all_indices.append(features['indices'])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to process file {row['pkl_path']}: {str(e)}\")\n",
    "                    failed_files.append(row['pkl_path'])\n",
    "                    continue\n",
    "                \n",
    "                # Periodic garbage collection\n",
    "                if idx % 100 == 0:\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Process PANNs embeddings\n",
    "            waveforms = np.stack(waveforms)\n",
    "            panns_clip, panns_embedding = self.process_panns_embeddings(waveforms)\n",
    "            del waveforms\n",
    "            gc.collect()\n",
    "            \n",
    "            # Save all features\n",
    "            self._save_features({\n",
    "                'yamnet': np.stack(yamnet_embeddings),\n",
    "                'vggish': np.stack(vggish_embeddings),\n",
    "                'panns_clip': panns_clip,\n",
    "                'panns_embedding': panns_embedding,\n",
    "                'mps': np.stack(mps_features) if mps_features else np.array([]),\n",
    "                'indices': pd.concat(all_indices, ignore_index=True)\n",
    "            })\n",
    "            \n",
    "            if failed_files:\n",
    "                logging.warning(f\"Failed to process {len(failed_files)} files\")\n",
    "                \n",
    "            logging.info(\"Processing completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _save_features(self, features: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Save extracted features to files\"\"\"\n",
    "        try:\n",
    "            for name, data in features.items():\n",
    "                if isinstance(data, pd.DataFrame):\n",
    "                    data.to_csv(self.output_dir / f'{name}_raw.csv', index=False)\n",
    "                else:\n",
    "                    np.save(self.output_dir / f'{name}_embedding.npy', data)\n",
    "                    \n",
    "            logging.info(f\"Features saved to {self.output_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving features: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Define paths\n",
    "        csv_path = \"/Users/huangjuhua/文档文稿/NYU/Time_Series/data/train_val_test_split_EMODB.csv\"\n",
    "        output_dir = \"/Users/huangjuhua/文档文稿/NYU/Time_Series/data/processed/embeddings\"\n",
    "        \n",
    "        # Initialize and run feature extraction\n",
    "        extractor = AudioEmbeddingExtractor(\n",
    "            csv_path=csv_path,\n",
    "            output_dir=output_dir,\n",
    "            use_gpu=False  # Set to True if GPU available\n",
    "        )\n",
    "        \n",
    "        extractor.process_dataset()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01408592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_finalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
